{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners."
      ],
      "metadata": {
        "id": "7SDjgi1WxXuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Boosting is an ensemble learning technique that combines multiple weak learners (models slightly better than random guessing) to create a strong learner.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "Models are trained sequentially\n",
        "\n",
        "Each new model focuses more on the errors made by previous models\n",
        "\n",
        "Misclassified samples receive higher importance\n",
        "\n",
        "Final prediction is a weighted combination of all models\n",
        "\n",
        "**Improvement mechanism:**\n",
        "\n",
        "Reduces bias\n",
        "\n",
        "Learns complex patterns\n",
        "\n",
        "Converts weak models into a highly accurate model"
      ],
      "metadata": {
        "id": "LhkA34t4y7Mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?"
      ],
      "metadata": {
        "id": "IrJWYuqexcET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature             | AdaBoost                                  | Gradient Boosting                       |\n",
        "| ------------------- | ----------------------------------------- | --------------------------------------- |\n",
        "| Error handling      | Increases weight of misclassified samples | Optimizes loss using gradients          |\n",
        "| Learning method     | Reweighting data                          | Gradient descent                        |\n",
        "| Loss function       | Exponential loss                          | Any differentiable loss                 |\n",
        "| Flexibility         | Less flexible                             | Highly flexible                         |\n",
        "| Overfitting control | Limited                                   | Strong control via learning rate, depth |\n"
      ],
      "metadata": {
        "id": "9-309U_rzBvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: How does regularization help in XGBoost?"
      ],
      "metadata": {
        "id": "ahw13VwNxdSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Regularization in XGBoost prevents overfitting by:\n",
        "\n",
        "Penalizing complex trees\n",
        "\n",
        "Controlling tree depth\n",
        "\n",
        "Limiting leaf weights\n",
        "\n",
        "**Key parameters:**\n",
        "\n",
        "gamma → penalizes number of splits\n",
        "\n",
        "lambda → L2 regularization\n",
        "\n",
        "alpha → L1 regularization\n",
        "\n",
        "max_depth, min_child_weight\n",
        "\n",
        "This leads to better generalization and robust performance."
      ],
      "metadata": {
        "id": "hxLu-sZLzC1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Why is CatBoost considered efficient for handling categorical data?"
      ],
      "metadata": {
        "id": "FF7njCTDxejT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **CatBoost:**\n",
        "\n",
        "Handles categorical variables natively\n",
        "\n",
        "Uses target encoding with permutation-based strategy\n",
        "\n",
        "Prevents target leakage\n",
        "\n",
        "No need for manual encoding (OneHot/LabelEncoding)\n",
        "\n",
        "**This makes CatBoost:**\n",
        "\n",
        "Faster\n",
        "\n",
        "More accurate\n",
        "\n",
        "Less prone to overfitting"
      ],
      "metadata": {
        "id": "bwq5FYbCzLbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "\n",
        " Use sklearn.datasets.fetch_california_housing() for regression tasks.\n"
      ],
      "metadata": {
        "id": "p-q3yk69xf5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Boosting is preferred when bias reduction is critical.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Credit risk & loan default prediction\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "Click-through rate (CTR) prediction\n",
        "\n",
        "Search ranking systems\n",
        "\n",
        "Bagging focuses more on variance reduction, while boosting excels at learning complex patterns."
      ],
      "metadata": {
        "id": "0AFnf3VMzR4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        " ● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy\n"
      ],
      "metadata": {
        "id": "NcBvUpf5xkSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"AdaBoost Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwAa1l5KzVTy",
        "outputId": "29d34e95-25a3-41f1-c4b9-10c29e2eb85a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        " ● Evaluate performance using R-squared score\n"
      ],
      "metadata": {
        "id": "DNLIu7VCxmzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load data\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NKIxYqnzXjL",
        "outputId": "6671917a-f054-4d3b-98d3-1767e9914e79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        " ● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Tune the learning rate using GridSearchCV\n",
        "\n",
        " ● Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "osUHBzbZxpYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(xgb, param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY21uA4JzaOb",
        "outputId": "a6338264-a83d-4ff6-b019-ade7dfc2fdac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [05:50:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [05:50:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [05:50:15] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [05:50:16] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [05:50:16] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [05:50:16] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [05:50:16] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [05:50:16] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [05:50:16] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [05:50:16] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.1}\n",
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a CatBoost Classifier\n",
        "\n",
        " ● Plot the confusion matrix using seaborn\n"
      ],
      "metadata": {
        "id": "N_-HxYinxscC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "MAboVqLdzcc7",
        "outputId": "25d2997a-1fb6-47c4-9692-91e47cb94f4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3007650956.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        " ● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model\n"
      ],
      "metadata": {
        "id": "5tTWp4dVxu17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Step 1: Data Preprocessing**\n",
        "\n",
        "Handle missing values:\n",
        "\n",
        "Median for numeric\n",
        "\n",
        "Mode or CatBoost handling for categorical\n",
        "\n",
        "Outlier treatment\n",
        "\n",
        "Feature scaling (for XGBoost)\n",
        "\n",
        " - **Step 2: Handling Categorical Features**\n",
        "\n",
        "Prefer CatBoost (native handling)\n",
        "\n",
        "If XGBoost:\n",
        "\n",
        "One-hot encoding\n",
        "\n",
        "Target encoding\n",
        "\n",
        " - **Step 3: Model Choice**\n",
        " | Scenario                  | Best Model |\n",
        "| ------------------------- | ---------- |\n",
        "| Many categorical features | CatBoost   |\n",
        "| Large structured dataset  | XGBoost    |\n",
        "| Simple baseline           | AdaBoost   |\n",
        "\n",
        "\n",
        " - **Step 4: Hyperparameter Tuning**\n",
        "\n",
        "GridSearchCV / RandomizedSearchCV\n",
        "\n",
        "Tune:\n",
        "\n",
        "learning_rate\n",
        "\n",
        "max_depth\n",
        "\n",
        "n_estimators\n",
        "\n",
        "scale_pos_weight (for imbalance)\n",
        "\n",
        " - **Step 5: Evaluation Metrics**\n",
        "\n",
        "Precision & Recall → minimize false negatives\n",
        "\n",
        "F1-score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "Accuracy is avoided due to imbalance.\n",
        "\n",
        " - **Step 6: Business Benefits**\n",
        "\n",
        "Reduced loan defaults\n",
        "\n",
        "Better risk segmentation\n",
        "\n",
        "Improved profitability\n",
        "\n",
        "Automated & explainable decisions\n",
        "\n",
        "Regulatory compliance"
      ],
      "metadata": {
        "id": "FZuTp547zsq6"
      }
    }
  ]
}